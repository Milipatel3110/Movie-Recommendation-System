{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------+\n",
      "|    id|          cast_names|          director|\n",
      "+------+--------------------+------------------+\n",
      "| 16420|  Laurence Fishburne|     Oliver Parker|\n",
      "| 31174|        Ian McKellen| Richard Loncraine|\n",
      "| 48750|  Jean-Paul Belmondo|    Claude Lelouch|\n",
      "| 46785|                NULL|      Jafar Panahi|\n",
      "|188588|                NULL|      Henry Jaglom|\n",
      "| 47475|     Ellen DeGeneres|       Nick Castle|\n",
      "| 52856|                NULL|      Diane Keaton|\n",
      "| 27985|                NULL|    Phillip Borsos|\n",
      "| 43475|                NULL|        Art Clokey|\n",
      "| 32631| Christopher Lambert|      J. F. Lawton|\n",
      "| 17402|Sarah Jessica Parker|     David Frankel|\n",
      "| 38722|                NULL|      Gregory Nava|\n",
      "| 41478|     Kadeem Hardison| Mario Van Peebles|\n",
      "|161495|          Peter Falk|       Peter Yates|\n",
      "| 32502|                NULL|Wallace Wolodarsky|\n",
      "|203119|                NULL|       Robert Wuhl|\n",
      "|171857|   Nicholas Turturro|  Michael Corrente|\n",
      "| 38129|     Martin Lawrence|   Thomas Schlamme|\n",
      "| 16771|      Taylor Nichols|     Whit Stillman|\n",
      "| 26203|     Madeleine Stowe|     Michael Apted|\n",
      "+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Load credits.csv\n",
    "credits_df = spark.read.csv(\"hdfs:///user/data/credits.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Define schema for parsing JSON in cast and crew columns\n",
    "cast_schema = ArrayType(StructType([\n",
    "    StructField(\"cast_id\", IntegerType(), True),\n",
    "    StructField(\"character\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "]))\n",
    "\n",
    "crew_schema = ArrayType(StructType([\n",
    "    StructField(\"job\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# Parse the JSON-like structure in the columns\n",
    "credits_df = credits_df \\\n",
    "    .withColumn(\"cast\", from_json(col(\"cast\"), cast_schema)) \\\n",
    "    .withColumn(\"crew\", from_json(col(\"crew\"), crew_schema))\n",
    "\n",
    "# Extract top 5 cast members (by name) as a list\n",
    "credits_df = credits_df.withColumn(\"cast_names\", col(\"cast.name\").getItem(0)).drop(\"cast\")\n",
    "\n",
    "# Explode the crew array to get individual crew members as rows\n",
    "crew_exploded_df = credits_df.withColumn(\"crew_member\", explode(col(\"crew\")))\n",
    "\n",
    "# Filter for director and select relevant columns\n",
    "credits_director_df = crew_exploded_df \\\n",
    "    .filter(col(\"crew_member.job\") == \"Director\") \\\n",
    "    .select(\"id\", \"cast_names\", col(\"crew_member.name\").alias(\"director\"))\n",
    "\n",
    "credits_director_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: int, keywords: string]\n",
      "+-----+--------------------+\n",
      "|   id|            keywords|\n",
      "+-----+--------------------+\n",
      "|  862|['jealousy', 'toy...|\n",
      "| 8844|\"['board game', '...|\n",
      "|15602|['fishing', 'best...|\n",
      "|31357|['based on novel'...|\n",
      "|11862|['baby', 'midlife...|\n",
      "|  949|['robbery', 'dete...|\n",
      "|11860|['paris', 'brothe...|\n",
      "|45325|                  []|\n",
      "| 9091|['terrorist', 'ho...|\n",
      "|  710|['cuba', 'falsely...|\n",
      "| 9087|['white house', '...|\n",
      "|12110|['dracula', 'spoof']|\n",
      "|21032|['wolf', 'dog-sle...|\n",
      "|10858|['usa president',...|\n",
      "| 1408|['exotic island',...|\n",
      "|  524|['poker', 'drug a...|\n",
      "| 4584|['bowling', 'base...|\n",
      "|    5|\"['hotel', \"\"new ...|\n",
      "| 9273|['africa', 'indig...|\n",
      "|11517|['brother brother...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, collect_list, concat_ws, regexp_replace\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"KeywordsFeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Load keywords.csv\n",
    "keywords_df = spark.read.csv(\"hdfs:///user/data/keywords.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(keywords_df)\n",
    "keywords_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data from keywords.csv:\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|id   |keywords                                                                                                                                   |\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|862  |['jealousy', 'toy', 'boy', 'friendship', 'friends', 'rivalry', 'boy next door', 'new toy', 'toy comes to life']                            |\n",
      "|8844 |\"['board game', 'disappearance', \"\"based on children's book\"\"                                                                              |\n",
      "|15602|['fishing', 'best friend', 'duringcreditsstinger', 'old men']                                                                              |\n",
      "|31357|['based on novel', 'interracial relationship', 'single mother', 'divorce', 'chick flick']                                                  |\n",
      "|11862|['baby', 'midlife crisis', 'confidence', 'aging', 'daughter', 'mother daughter relationship', 'pregnancy', 'contraception', 'gynecologist']|\n",
      "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Exploded Keywords Data after cleaning:\n",
      "+--------+------------+\n",
      "|movie_id|keyword_name|\n",
      "+--------+------------+\n",
      "|862     |jealousy    |\n",
      "|862     |toy         |\n",
      "|862     |boy         |\n",
      "|862     |friendship  |\n",
      "|862     |friends     |\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Final Transformed Keywords Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|movie_id|keyword_names                                                                                                                                                                                                                                                                                                          |\n",
      "+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|3       |salesclerk,helsinki,garbage,independent film                                                                                                                                                                                                                                                                           |\n",
      "|6       |chicago,drug dealer,boxing match,escape,one night                                                                                                                                                                                                                                                                      |\n",
      "|12      |father son relationship,harbor,underwater,fish tank,great barrier reef,missing child,aftercreditsstinger,duringcreditsstinger,short term memory loss,clownfish,father son reunion,protective father                                                                                                                    |\n",
      "|13      |vietnam veteran,hippie,mentally disabled,running,based on novel,vietnam,vietnam war,friendship,love,family relationships,bully,mother son relationship,military,hugging,shrimping,wounded soldier,flashback,park bench,amputee                                                                                         |\n",
      "|16      |individual,dancing,usa,robbery,factory worker,secret,factory,small town,dance,blindness and impaired vision,death penalty,immigrant,eye operation,eyesight,fantasy,naivety,self-abandonment,hereditary disease,loss of eyesight,czech,dying and death,van,friendship,murder,teacher,debt,execution,crime,police officer|\n",
      "+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, collect_list, concat_ws, regexp_replace, split\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"KeywordsFeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Load keywords.csv\n",
    "keywords_df = spark.read.csv(\"hdfs:///user/data/keywords.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display initial data to check its structure\n",
    "print(\"Initial Data from keywords.csv:\")\n",
    "keywords_df.show(5, truncate=False)\n",
    "\n",
    "# Clean up the `keywords` column by removing unwanted characters\n",
    "keywords_df = keywords_df.withColumn(\n",
    "    \"keywords\",\n",
    "    regexp_replace(col(\"keywords\"), r\"^\\[|\\]$\", \"\")  # Remove square brackets at start and end\n",
    ")\n",
    "keywords_df = keywords_df.withColumn(\n",
    "    \"keywords\",\n",
    "    regexp_replace(col(\"keywords\"), r\"'|\\\"\", \"\")  # Remove single and double quotes\n",
    ")\n",
    "\n",
    "# Split the cleaned string into an array\n",
    "keywords_df = keywords_df.withColumn(\"keywords\", split(col(\"keywords\"), \", \"))\n",
    "\n",
    "# Explode the keywords array to get each keyword in a separate row\n",
    "keywords_exploded_df = keywords_df.withColumn(\"keyword_name\", explode(col(\"keywords\")))\n",
    "\n",
    "# Select only movie_id and keyword name\n",
    "keywords_exploded_df = keywords_exploded_df.select(\n",
    "    col(\"id\").alias(\"movie_id\"),\n",
    "    col(\"keyword_name\")\n",
    ")\n",
    "\n",
    "# Check exploded keyword data\n",
    "print(\"Exploded Keywords Data after cleaning:\")\n",
    "keywords_exploded_df.show(5, truncate=False)\n",
    "\n",
    "# Group by movie_id and collect all keyword names into a list\n",
    "keywords_aggregated_df = keywords_exploded_df.groupBy(\"movie_id\").agg(\n",
    "    collect_list(\"keyword_name\").alias(\"keyword_names\")\n",
    ")\n",
    "\n",
    "# Convert the array of keyword names to a comma-separated string\n",
    "keywords_aggregated_df = keywords_aggregated_df.withColumn(\n",
    "    \"keyword_names\", concat_ws(\",\", col(\"keyword_names\"))\n",
    ")\n",
    "\n",
    "# Display final result for verification\n",
    "print(\"Final Transformed Keywords Data:\")\n",
    "keywords_aggregated_df.show(5, truncate=False)\n",
    "\n",
    "# Save the cleaned and transformed data to HDFS\n",
    "output_path = \"hdfs:///user/data/cleaned_keywords.csv\"\n",
    "keywords_aggregated_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Hot Encoded Keywords Data:\n",
      "+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|movie_id|keyword_features                                                                                                                                                                                                                     |\n",
      "+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2       |(20107,[1,22,267,368,552,1495,2599],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                                   |\n",
      "|11      |(20107,[443,508,1071,1288,1713,2562,3014,3081,3094,4009,4823,6677,7259,7407,7635,11056],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                           |\n",
      "|14      |(20107,[7,12,51,83,87,94,166,212,269,358,369,471,692,1060,1133,1727,1753,2084,3451,4024,4983],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                 |\n",
      "|18      |(20107,[11,31,55,123,167,197,324,363,379,508,566,578,611,970,1336,1353,1356,2111,2575,3338,3860,4272,7459,7955,9855,20070],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|21      |(20107,[895,1997,3874],[1.0,1.0,1.0])                                                                                                                                                                                                |\n",
      "+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "TF-IDF Encoded Keywords Data:\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|movie_id|tfidf_features                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2       |(20107,[1,22,267,368,552,1495,2599],[2.800935124945572,4.530607460622015,5.975290139980061,6.18585490908741,6.516096595957987,7.349005718893091,7.859831342659081])                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|11      |(20107,[443,508,1071,1288,1713,2562,3014,3081,3094,4009,4823,6677,7259,7407,7635,11056],[6.3193863017119325,6.432714987018936,7.012533482271878,7.2092437765179325,7.454366234550917,7.859831342659081,8.042152899453036,8.042152899453036,8.042152899453036,8.398827843391768,8.552978523219027,8.95844363132719,8.95844363132719,8.95844363132719,8.95844363132719,9.246125703778972])                                                                                                                                                                                                                          |\n",
      "|14      |(20107,[7,12,51,83,87,94,166,212,269,358,369,471,692,1060,1133,1727,1753,2084,3451,4024,4983],[3.9085876240776543,4.047628672513146,4.937566220986963,5.1972435156336285,5.20893955539682,5.2327502040905385,5.6085395440525865,5.75977051377651,5.975290139980061,6.1703507225514445,6.201603266055549,6.37444607889496,6.681176346317435,7.012533482271878,7.0866414544256,7.511524648390866,7.511524648390866,7.705680662831823,8.147513415110863,8.398827843391768,8.552978523219027])                                                                                                                        |\n",
      "|18      |(20107,[11,31,55,123,167,197,324,363,379,508,566,578,611,970,1336,1353,1356,2111,2575,3338,3860,4272,7459,7955,9855,20070],[4.0203790300657705,4.717116878756444,4.969459584762917,5.41026405931639,5.617350173734741,5.700347093305709,6.1106314878498225,6.1703507225514445,6.2176036074019905,6.432714987018936,6.538075502676762,6.538075502676762,6.583537876753519,6.943540610784926,7.253695539088766,7.253695539088766,7.253695539088766,7.705680662831823,7.859831342659081,8.147513415110863,8.265296450767245,8.398827843391768,8.95844363132719,8.95844363132719,9.246125703778972,9.651590811887136])|\n",
      "|21      |(20107,[895,1997,3874],[6.879002089647355,7.636687791344872,8.265296450767245])                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+--------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, col, array\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"KeywordsFeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Load cleaned keywords data\n",
    "keywords_df = spark.read.csv(\"hdfs:///user/data/cleaned_keywords.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Split the comma-separated keyword names back into an array format\n",
    "keywords_df = keywords_df.withColumn(\"keyword_names\", split(col(\"keyword_names\"), \",\"))\n",
    "\n",
    "# Step 1: Remove NULL and empty keyword arrays\n",
    "keywords_df = keywords_df.filter((col(\"keyword_names\").isNotNull()) & (col(\"keyword_names\") != array()))\n",
    "\n",
    "# Step 2: Multi-Hot Encoding using CountVectorizer\n",
    "\n",
    "# Use CountVectorizer to create multi-hot encoding\n",
    "cv = CountVectorizer(inputCol=\"keyword_names\", outputCol=\"keyword_features\", binary=True)\n",
    "cv_model = cv.fit(keywords_df)\n",
    "multi_hot_keywords_df = cv_model.transform(keywords_df)\n",
    "\n",
    "# Display the multi-hot encoded features\n",
    "print(\"Multi-Hot Encoded Keywords Data:\")\n",
    "multi_hot_keywords_df.select(\"movie_id\", \"keyword_features\").show(5, truncate=False)\n",
    "\n",
    "# Step 3: TF-IDF Transformation\n",
    "\n",
    "# Compute Term Frequency\n",
    "tf_keywords_df = cv_model.transform(keywords_df)\n",
    "\n",
    "# Apply IDF to get TF-IDF values\n",
    "idf = IDF(inputCol=\"keyword_features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(tf_keywords_df)\n",
    "tfidf_keywords_df = idf_model.transform(tf_keywords_df)\n",
    "\n",
    "# Display the TF-IDF features\n",
    "print(\"TF-IDF Encoded Keywords Data:\")\n",
    "tfidf_keywords_df.select(\"movie_id\", \"tfidf_features\").show(5, truncate=False)\n",
    "\n",
    "# Save the transformed data if needed\n",
    "tfidf_output_path = \"hdfs:///user/data/tfidf_keywords.csv\"\n",
    "\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|imdbId|tmdbId|\n",
      "+-------+------+------+\n",
      "|      1|114709|   862|\n",
      "|      2|113497|  8844|\n",
      "|      3|113228| 15602|\n",
      "|      4|114885| 31357|\n",
      "|      5|113041| 11862|\n",
      "|      6|113277|   949|\n",
      "|      7|114319| 11860|\n",
      "|      8|112302| 45325|\n",
      "|      9|114576|  9091|\n",
      "|     10|113189|   710|\n",
      "+-------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CombineLinks\").getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "links_df = spark.read.csv(\"hdfs:///user/data/links.csv\", header=True, inferSchema=True)\n",
    "links_small_df = spark.read.csv(\"hdfs:///user/data/links_small.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 1: Handle Missing Values in `tmdbId`\n",
    "# Replace null values in 'tmdbId' with -1 as placeholder, then cast to integer\n",
    "links_df = links_df.fillna({\"tmdbId\": -1}).withColumn(\"tmdbId\", col(\"tmdbId\").cast(\"int\"))\n",
    "links_small_df = links_small_df.fillna({\"tmdbId\": -1}).withColumn(\"tmdbId\", col(\"tmdbId\").cast(\"int\"))\n",
    "\n",
    "# Rename `tmdbId` in `links_small_df` to avoid ambiguity during join\n",
    "links_small_df = links_small_df.withColumnRenamed(\"tmdbId\", \"tmdbId_small\")\n",
    "\n",
    "# Step 2: Perform an Outer Join\n",
    "# Use coalesce to combine `tmdbId` values from both datasets, prioritizing `tmdbId_small` if available\n",
    "combined_links_df = links_df.join(\n",
    "    links_small_df,\n",
    "    on=[\"movieId\", \"imdbId\"],\n",
    "    how=\"outer\"\n",
    ").select(\n",
    "    col(\"movieId\"),\n",
    "    col(\"imdbId\"),\n",
    "    coalesce(col(\"tmdbId_small\"), col(\"tmdbId\")).alias(\"tmdbId\")\n",
    ")\n",
    "\n",
    "# Step 3: Remove placeholder values if needed (optional)\n",
    "# If you don’t want placeholder -1 values in the final combined DataFrame, you can filter them out\n",
    "combined_links_df = combined_links_df.filter(col(\"tmdbId\") != -1)\n",
    "\n",
    "# Step 4: Save the Combined Data\n",
    "combined_output_path = \"hdfs:///user/data/combined_links.csv\"\n",
    "combined_links_df.write.csv(combined_output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# Show combined data for verification\n",
    "combined_links_df.show(10)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movie_recommender_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
